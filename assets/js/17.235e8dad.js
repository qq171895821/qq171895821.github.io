(window.webpackJsonp=window.webpackJsonp||[]).push([[17],{607:function(_,l,v){"use strict";v.r(l);var e=v(0),a=Object(e.a)({},(function(){var _=this,l=_.$createElement,v=_._self._c||l;return v("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[v("div",{staticClass:"custom-block tip"},[v("p",[_._v("XGBoost重要参数说明")])]),_._v(" "),v("h2",{attrs:{id:"xbgboost参数说明"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#xbgboost参数说明"}},[_._v("#")]),_._v(" XBGboost参数说明")]),_._v(" "),v("p",[_._v("XGBoost参数繁多，所以如果要调参，就一定要知道每个参数的作用意义。")]),_._v(" "),v("h3",{attrs:{id:"xgboost的参数定义"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#xgboost的参数定义"}},[_._v("#")]),_._v(" XGBoost的参数定义")]),_._v(" "),v("p",[_._v("XGBoost的作者将参数归纳成三种类型，分别是：")]),_._v(" "),v("ol",[v("li",[_._v("通用参数(General parameters):与我们所使用的booster相关，通常是树模型或者线性模型，宏观上对函数进行调控。")]),_._v(" "),v("li",[_._v("Booster参数(Booster parameters):根据你所选择的booster，控制每一次迭代的参数。")]),_._v(" "),v("li",[_._v("学习任务函数(Learning task parameters):根据你的任务情况，选择不同的模型可能会有不同的参数，但它们都是控制训练任务的表现。")])]),_._v(" "),v("h3",{attrs:{id:"通用参数-general-paremeters"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#通用参数-general-paremeters"}},[_._v("#")]),_._v(" 通用参数(General paremeters):")]),_._v(" "),v("ul",[v("li",[_._v("booster[default=gbtree]:\n"),v("ul",[v("li",[_._v("gbtree:基于树的模型")]),_._v(" "),v("li",[_._v("gblinear:线性模型")])])]),_._v(" "),v("li",[_._v("Verbosity[default=1]:\n"),v("ul",[v("li",[_._v("当这个参数的值为1时，输出模型的很多信息，这有助于帮助我们更好的理解模型，所以最好为1.")]),_._v(" "),v("li",[_._v("当这个参数为0时等价于Silent=1，不会输出任何信息。")])])]),_._v(" "),v("li",[_._v("nthread[默认为最大可用线程数（如果未设置）]:\n"),v("ul",[v("li",[_._v("用于运行XGBoost的并行线程数。")])])])]),_._v(" "),v("h3",{attrs:{id:"booster参数-booster-parameters"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#booster参数-booster-parameters"}},[_._v("#")]),_._v(" Booster参数(Booster parameters):")]),_._v(" "),v("p",[_._v("在实际使用XGBoost中，"),v("code",[_._v("gbtree")]),_._v("模型的性能与表现远远好于"),v("code",[_._v("linear")]),_._v("模型，因此我们主要讨论下Tree Booster模型的参数。")]),_._v(" "),v("ul",[v("li",[_._v("eta[default=0.3]:\n"),v("ul",[v("li",[_._v("在sklearn中成为learning_rate")]),_._v(" "),v("li",[_._v("在每次迭代之后，可以直接得到每个叶子节点的权重，而使权值显小来提升模型的鲁棒性，可以防止过拟合。")]),_._v(" "),v("li",[_._v("最佳值范围:[0.01, 0.2]")])])]),_._v(" "),v("li",[_._v("gamma[default=0]:\n"),v("ul",[v("li",[_._v("也称min_split_loss")]),_._v(" "),v("li",[_._v("在树的结点进行进一步划分时所需的最小损失的减少值，在划分时最小损失值大于Gamma时，才会划分这个点，Gamma制定了节点分裂所需的最小损失函数下降值。")]),_._v(" "),v("li",[_._v("Gamma值越大，算法就越保守，因此这个值是需要调节的。")]),_._v(" "),v("li",[_._v("范围：[0,∞]")])])]),_._v(" "),v("li",[_._v("max_depth[default=6]:\n"),v("ul",[v("li",[_._v("树的最大深度，增加这个值将使模型更为复杂，并容易过拟合")]),_._v(" "),v("li",[_._v("可以使用cv函数调优")]),_._v(" "),v("li",[_._v("最佳值范围：[3,10]")])])]),_._v(" "),v("li",[_._v("min_child_weight[default=1]:\n"),v("ul",[v("li",[_._v("子节点的所有样本hessian"),v("code",[_._v("h")]),_._v("的和的最小值，如果在树的划分过程中样本的"),v("code",[_._v("h")]),_._v("之和的最小值小于min_child_weight，那么不对该节点进行进一步的划分")]),_._v(" "),v("li",[_._v("min_child_weight越大，算法越是保守")]),_._v(" "),v("li",[_._v("这个参数需要使用cv函数调优")])])]),_._v(" "),v("li",[_._v("max_delta_step[default=0]:\n"),v("ul",[v("li",[_._v("这个参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守")]),_._v(" "),v("li",[_._v("通常，这个参数不需要设置。但是当各类样本十分不平衡时，它对逻辑回归是很有帮助的")]),_._v(" "),v("li",[_._v("将其设置为1-10可能有助于控制更新")])])]),_._v(" "),v("li",[_._v("subsample[default=1]:\n"),v("ul",[v("li",[_._v("假设设置为0.5，意味着xgboost将在树的增长之前随机抽取一般的数据用于训练")]),_._v(" "),v("li",[_._v("可以防止过拟合")]),_._v(" "),v("li",[_._v("在每次提成迭代时都会采样一次")]),_._v(" "),v("li",[_._v("最佳值范围：[0.5,1]")])])]),_._v(" "),v("li",[_._v("colsample_bytree:\n"),v("ul",[v("li",[_._v("建立每一颗树的时候对样本的列（特征）进行采样，用于建立下一颗树")]),_._v(" "),v("li",[_._v("可以防止过拟合")]),_._v(" "),v("li",[_._v("在每次提升迭代时都会采样一次")]),_._v(" "),v("li",[_._v("最佳值范围：[0.5,1]")])])]),_._v(" "),v("li",[_._v("colsmaple_bylevel:\n"),v("ul",[v("li",[_._v("建立每一棵树的时候，对每一层的样本的列（特征）进行采样")]),_._v(" "),v("li",[_._v("可以防止过拟合")]),_._v(" "),v("li",[_._v("在每次提升迭代的时候都会采样一次")]),_._v(" "),v("li",[_._v("最佳值范围：[0.5,1]")])])]),_._v(" "),v("li",[_._v("lambda[default=1]:\n"),v("ul",[v("li",[_._v("也称reg_lambda")]),_._v(" "),v("li",[_._v("权重的L2正则化项(与Ridge regression类似)")]),_._v(" "),v("li",[_._v("增加这个值可以使模型更为保守")])])]),_._v(" "),v("li",[_._v("alpha[default=1]:\n"),v("ul",[v("li",[_._v("也称reg_alpha")]),_._v(" "),v("li",[_._v("权重的L1正则化项(与Lasso regression类似)")]),_._v(" "),v("li",[_._v("可以产生稀疏矩阵，加快算法收敛速度")])])]),_._v(" "),v("li",[_._v("scale_pos_weight[detault=1]:\n"),v("ul",[v("li",[_._v("控制正负权重的平衡，对于不平衡类很有用")])])])]),_._v(" "),v("h3",{attrs:{id:"学习任务函数-learning-task-parameters"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#学习任务函数-learning-task-parameters"}},[_._v("#")]),_._v(" 学习任务函数(Learning task parameters):")]),_._v(" "),v("ul",[v("li",[_._v("objective[default=reg:linear]:\n"),v("ul",[v("li",[_._v("reg:linear:线性回归")]),_._v(" "),v("li",[_._v("reg:logistic:逻辑回归")]),_._v(" "),v("li",[_._v("binary:logistic:二分类的逻辑回归，返回预测的概率（不是类别）")]),_._v(" "),v("li",[_._v("multi:softmax:使用softmax目标函数让XGBoost解决多分类问题，我们需要输入类别的个数")]),_._v(" "),v("li",[_._v("multi:softprob:与SoftMax相同，但输出一个ndata ✖ nclass的向量，该向量可以进一步整形为ndata ✖ nclass矩阵。结果包含属于每个类的每个数据点的预测概率。")]),_._v(" "),v("li",[_._v("rank:pairwise:使用lambdamart在最小化成对损失的情况下执行成对排名")])])]),_._v(" "),v("li",[_._v("base_score:\n"),v("ul",[v("li",[_._v("所有实例的初始预测得分，全局偏差")]),_._v(" "),v("li",[_._v("对于足够的迭代次数，更改次之不会产生太大的影响。")])])]),_._v(" "),v("li",[_._v("eval_metric:\n"),v("ul",[v("li",[_._v("验证数据的评估指标，将根据目标分配默认指标(回归的RMSE，分类的误差，排名的平均精度)")])])]),_._v(" "),v("li",[_._v("seed[default=0]:\n"),v("ul",[v("li",[_._v("随机数种子")])])])])])}),[],!1,null,null,null);l.default=a.exports}}]);